{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "## [DataCamp Course Dashboard](https://www.datacamp.com/courses/deep-learning-in-python)\n",
    "\n",
    "### Imagine you work for a bank\n",
    "- You need to predict how many transactions each customer will make next year\n",
    "- You have features like `age`, `bank_balance`, `retirement_status`, etc.\n",
    "\n",
    "#### How would a linear regression model work through this problem?\n",
    "- The LR model would look at the influence of each feature individually and not the interactions between the features. This is not ideal from an intutive standpoint.\n",
    "\n",
    "# Interactions\n",
    "- Neural networks account for interactions really well\n",
    "- Deep learning uses especially powerful neural networks\n",
    "\n",
    "# Course structure\n",
    "- First two chapters focus on conceptual knowledge\n",
    "    - Debug and tune deep learning models on conventional prediction problems\n",
    "    - Lay the foundation for progressing towards modern applications\n",
    "    \n",
    "## Build deep learning models with keras\n",
    "```python\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "predictors = np.loadtxt('predictors_data.csv', delimiter=',')\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "## Deep learning models capture interactions\n",
    "- We generate a function that describes the interaction of variables and use that function to predict the outcome of the response variable.\n",
    "- The input layer describes our predictive features (`age`, `bank_balance`, `retirement_status`, etc)\n",
    "- The output layer describes our response variable or what we want to predict.\n",
    "- All other layers are called the hidden layers because these are non-empirical values that are calculated by the model to account for the interaction between variables in the input layer and more accurately predict the output layer.\n",
    "\n",
    "# Forward propagation\n",
    "- First step of the **forward propagation algorithm** is to draw lines between the input layer and the nodes of the hidden layer.\n",
    "- Each line is given a weight to account for the strength of that interaction.\n",
    "- Next, each input layer value is multiplied by its line weight and added to all other input layer calculations corresponding to that node.\n",
    "- This process to repeated to draw lines, assign weights, and apply calculations to yeild the output layer value.\n",
    "\n",
    "    - Multiply - add process\n",
    "    - Dot product\n",
    "    - Forward propagation for one data point at a time\n",
    "    - Output is the prediction for that data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer values:  [5 1]\n",
      "output value:  9\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation code\n",
    "\n",
    "import numpy as np\n",
    "input_data = np.array([2, 3])\n",
    "weights = {'node_0': np.array([1, 1]),\n",
    "          'node_1': np.array([-1, 1]),\n",
    "          'output': np.array([2, -1])}\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "print('hidden layer values: ', hidden_layer_values)\n",
    "\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "print('output value: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3, 5])\n",
    "weights = {'node_0': np.array([2, 4]), \n",
    "           'node_1': np.array([ 4, -5]), \n",
    "           'output': np.array([2, 7])}\n",
    "\n",
    "# Calculate node 0 value: node_0_value\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "\n",
    "# Calculate node 1 value: node_1_value\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_value, node_1_value])\n",
    "\n",
    "# Calculate output: output\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "- These functions allow the model to capture nonlinearities.\n",
    "- Applied to node inputs to produce node output\n",
    "- `tanh()` was a very popular activation function\n",
    "- Today, ReLU (Rectified Linear Activation) is the industry standard activation function\n",
    "$$RELU(x) = \\left\\{\\begin{matrix}\n",
    "0 \\text{ if } x<0\\\\ \n",
    "x \\text{ if } x\\geq 0\n",
    "\\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer outputs:  [0.99505475 0.99999997]\n",
      "output value:  0.9901095378334199\n"
     ]
    }
   ],
   "source": [
    "# Activation function code\n",
    "\n",
    "import numpy as np\n",
    "input_data = np.array([-1, 2])\n",
    "weights = {'node_0': np.array([3, 3]),\n",
    "          'node_1': np.array([1, 5]),\n",
    "          'output': np.array([2, -1])}\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "print('hidden layer outputs: ', hidden_layer_outputs)\n",
    "\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "print('output value: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output layer value: 52 \n",
      "Without the activation function, this prediction would be negative.\n"
     ]
    }
   ],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(input, 0)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "input_data = np.array([3, 5])\n",
    "weights = {'node_0': np.array([2, 4]), \n",
    "           'node_1': np.array([ 4, -5]), \n",
    "           'output': np.array([2, 7])}\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print('output layer value:', model_output, '\\nWithout the activation function, \\\n",
    "this prediction would be negative.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "input_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n",
    "weights = {'node_0': np.array([2, 4]), \n",
    "           'node_1': np.array([ 4, -5]), \n",
    "           'output': np.array([2, 7])}\n",
    "\n",
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper networks\n",
    "## Multiple hidden layers\n",
    "## Representation learning\n",
    "- Deep networks internally build representations of patterns in the data.\n",
    "- Partially replace the need for feature engineering\n",
    "- Subsequent layers build increasingly sophisticated representations of raw data\n",
    "\n",
    "# Deep learning\n",
    "- Modeler doesn't need to specify the interactions\n",
    "- When you train the model, the neural network gets weights that find the relevant patterns to make better predictions\n",
    "\n",
    "*Identity function* - each node's output will be the same as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output value is: 182\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3, 5])\n",
    "weights = {'node_0_0': np.array([2, 4]),\n",
    "           'node_0_1': np.array([ 4, -5]),\n",
    "           'node_1_0': np.array([-1,  2]),\n",
    "           'node_1_1': np.array([1, 2]),\n",
    "           'output': np.array([2, 7])}\n",
    "\n",
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print('The output value is:', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The need for optimization\n",
    "## Loss function\n",
    "- Aggregates errors in predictions from many data points into single number\n",
    "- Measure of model's predictive performance\n",
    "- Mean Squared Error loss function\n",
    "- Goal: Find the weights that give the lowest value of the loss function\n",
    "- Gradient descent is the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_network_weights(input_data_point, weights):\n",
    "    '''Takes input data and a dictionary of weights and\n",
    "    returns the neural network output layer value.\n",
    "    '''\n",
    "    \n",
    "    node_0_input = (input_data_point * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "    \n",
    "    node_1_input = (input_data_point * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "    \n",
    "    hidden_layer_values = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    input_to_final_layer = (hidden_layer_values * weights['output']).sum()\n",
    "    \n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    return(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pull this code from DataCamp, I imported the `inspect` package and used the `getsource()` function passing in `predict_with_network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def predict_with_network_weights(input_data_point, weights):\\n    '''Takes input data and a dictionary of weights and\\n    returns the neural network output layer value.\\n    '''\\n    \\n    node_0_input = (input_data_point * weights['node_0']).sum()\\n    node_0_output = relu(node_0_input)\\n    \\n    node_1_input = (input_data_point * weights['node_1']).sum()\\n    node_1_output = relu(node_1_input)\\n    \\n    hidden_layer_values = np.array([node_0_output, node_1_output])\\n    \\n    input_to_final_layer = (hidden_layer_values * weights['output']).sum()\\n    \\n    model_output = relu(input_to_final_layer)\\n    \\n    return(model_output)\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getsource(predict_with_network_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# The data point you will make a prediction for\n",
    "input_data = np.array([0, 3])\n",
    "\n",
    "# Sample weights\n",
    "weights_0 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 2],\n",
    "             'output': [1, 1]\n",
    "            }\n",
    "\n",
    "# The actual target value, used to calculate the error\n",
    "target_actual = 3\n",
    "\n",
    "# Make prediction using original weights\n",
    "model_output_0 = predict_with_network_weights(input_data, weights_0)\n",
    "\n",
    "# Calculate error: error_0\n",
    "error_0 = model_output_0 - target_actual\n",
    "\n",
    "# Create weights that cause the network to make perfect prediction (3): weights_1\n",
    "weights_1 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 0],\n",
    "             'output': [1, 1]\n",
    "            }\n",
    "\n",
    "# Make prediction using new weights: model_output_1\n",
    "model_output_1 = predict_with_network_weights(input_data, weights_1)\n",
    "\n",
    "# Calculate error: error_1\n",
    "error_1 = target_actual - model_output_1\n",
    "\n",
    "# Print error_0 and error_1\n",
    "print(error_0)\n",
    "print(error_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [np.array([0, 3]), \n",
    "              np.array([1, 2]), \n",
    "              np.array([-1, -2]), \n",
    "              np.array([4, 0])]\n",
    "weights_0 = {'node_0': np.array([2, 1]), \n",
    "             'node_1': np.array([1, 2]), \n",
    "             'output': np.array([1, 1])}\n",
    "weights_1 = {'node_0': np.array([2, 1]),\n",
    "             'node_1': np.array([1. , 1.5]),\n",
    "             'output': np.array([1. , 1.5])}\n",
    "target_actuals = [1, 3, 5, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 3683.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error with weights_0: 37.500000\n",
      "Mean squared error with weights_1: 49.890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create model_output_0 \n",
    "model_output_0 = []\n",
    "# Create model_output_1\n",
    "model_output_1 = []\n",
    "\n",
    "# Loop over input_data\n",
    "for row in tqdm(input_data):\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network_weights(row, weights_0))\n",
    "    \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network_weights(row, weights_1))\n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_0 = mean_squared_error(target_actuals, model_output_0)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_1 = mean_squared_error(target_actuals, model_output_1)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "- If the slope is positive:\n",
    "    - Going opposite the slope means moving to lower numbers\n",
    "    - Subtract the slope from the current value\n",
    "    - Too big a step might lead us astray\n",
    "- Solution: learning rate\n",
    "    - Update each weight by subtracting **learning rate \\* slope** (learning rates are usually around 0.01)\n",
    "    \n",
    "# Slope calculation example\n",
    "- To calculate the slope for a weight, need to multiply:\n",
    "    - Slope of the loss function with respect to (w.r.t.) value at the node we feed into\n",
    "        - Slope of mean-squared loss function w.r.t. prediction\n",
    "        - 2 \\* (Predicted Value - Actual Value) = 2 \\* Error\n",
    "    - The value of the node that feeds into our weight\n",
    "    - Slope of the activation function w.r.t. value we feed into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.array([1, 2])\n",
    "input_data = np.array([3, 4])\n",
    "target = 6\n",
    "learning_rate = 0.01\n",
    "preds = (weights * input_data).sum()\n",
    "error = preds - target\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 40])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = 2 * input_data * error\n",
    "\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    }
   ],
   "source": [
    "weights_updated = weights - learning_rate * gradient\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "error_updated = preds_updated - target\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 28 42]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([1, 2, 3])\n",
    "weights = np.array([0, 2, 1])\n",
    "target = 0\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Print the slope\n",
    "print(slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original weighted error: 7\n",
      "mse updated error: 5.04\n"
     ]
    }
   ],
   "source": [
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - (learning_rate * slope)\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print('original weighted error:', error)\n",
    "\n",
    "# Print the updated error\n",
    "print('mse updated error:', error_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope(input_data, target, weights):\n",
    "    error = get_error(input_data, target, weights)\n",
    "    slope = 2 * input_data * error\n",
    "    return(slope)\n",
    "\n",
    "def get_mse(input_data, target, weights):\n",
    "    errors = get_error(input_data, target, weights)\n",
    "    mse = np.mean(errors**2)\n",
    "    return(mse)\n",
    "\n",
    "def get_error(input_data, target, weights):\n",
    "    preds = (weights * input_data).sum()\n",
    "    error = preds - target\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 8658.76it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_updates = 20\n",
    "mse_hist = []\n",
    "\n",
    "# Iterate over the number of updates\n",
    "for i in tqdm(range(n_updates)):\n",
    "    # Calculate the slope: slope\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "    \n",
    "    # Update the weights: weights\n",
    "    weights = weights - 0.01 * slope\n",
    "    \n",
    "    # Calculate mse with new weights: mse\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "    \n",
    "    # Append the mse to mse_hist\n",
    "    mse_hist.append(mse)\n",
    "\n",
    "# Plot the mse history\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "- Calculate slopes in more complex models\n",
    "- Allows gradient descent to update all weights in neural network (by getting gradients for all weights)\n",
    "- Comes from chain rule of calculus\n",
    "- Important to understand the process, but you will generally use a library that implements this\n",
    "\n",
    "# Process\n",
    "- Trying to estimate the slope of the loss function w.r.t. each weight\n",
    "- Do forward propagation to calculate predictions and errors\n",
    "    - Go back one layer at a time\n",
    "    - Gradients for weight is product of:\n",
    "        1. Node value feeding into that weight\n",
    "        2. Slope of loss function w.r.t. node it feeds into\n",
    "        3. Slope of activation function at the node it feeds into\n",
    "    \n",
    "- Need to also keep track of the slpes of the loss function w.r.t. node values\n",
    "- Slope of node values are the sum of the slopes for all weights that come out of them\n",
    "\n",
    "- Each time you generate predictions using forward propagation, you update the weights using backward propagation.\n",
    "\n",
    "# Calculating slopes associated with any weight\n",
    "- Gradient for weight is product of:\n",
    "    1. Node value feeding into that weight\n",
    "    2. Slope of activation function for the node being fed into\n",
    "    3. Slope of loss function w.r.t. output node\n",
    "    \n",
    "# Recap\n",
    "- Start at some random set of weights\n",
    "- Use forward propagation to make a prediction\n",
    "- Use backward propagation to calculate the slope of the loss function w.r.t. each weight\n",
    "- Multiply that slope by the learning rate, and subtract from the current weights\n",
    "- Keep going with that cycle until we get to a flat part\n",
    "\n",
    "# Stochastic gradient descent\n",
    "- It is common to calculate slopes on only a subset of the data ('batch')\n",
    "- Use a different batch of data to calculate the next update\n",
    "- Start over from the beginning once all data is used\n",
    "- Each time through the training data is called an epoch\n",
    "- When slopes are calculated on one batch at a time:\n",
    "    - stochastic gradient descent\n",
    "    \n",
    "# Creating a keras model\n",
    "## Model building steps\n",
    "- Specify Architecture\n",
    "- Compile\n",
    "- Fit\n",
    "- Predict\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "predictors = np.loadtxt('predictors_data.csv',\n",
    "                        delimiter=',')\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', \n",
    "                input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = np.loadtxt('hourly_wages.csv', delimiter=',', skiprows=1, usecols=(1, 2, 3, 4, 5, 6, 7, 8, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and fitting a model\n",
    "- Specify the optimizer\n",
    "    - Controls the learning rate\n",
    "    - Many options and mathematically comples\n",
    "    - \"Adam\" is usually a good choice\n",
    "- Loss function\n",
    "    - \"mean_squared_error\" common for regression\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "# Fitting a model\n",
    "- Applying backpropagation and gradient descent with your data to update the weights\n",
    "- Scaling data before fitting can ease optimization\n",
    "\n",
    "```python\n",
    "model.fit(predictors, target)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.loadtxt('hourly_wages.csv', delimiter=',', skiprows=1, usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "534/534 [==============================] - 0s 693us/step - loss: 142.1494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2d232fd0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "- 'categarical_crossentropy' loss function\n",
    "- Similar to log loss: Lower is better\n",
    "- Add metrics = `['accuracy']` to compile step for easy-to-understand diagnostics\n",
    "- Output layer has separate node for each possible outcome and uses `softmax` activation\n",
    "\n",
    "```python\n",
    "from keras.utils import to_categorical\n",
    "data = pd.read_csv('basketball_shot_log.csv')\n",
    "predictors = data.drop(['shot_results']), axis=1).as_matrix()\n",
    "target = to_categorical(data.shot_result)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', \n",
    "                input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>13.002015</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         survived      pclass         age       sibsp       parch        fare  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208   \n",
       "std      0.486592    0.836071   13.002015    1.102743    0.806057   49.693429   \n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    2.000000   22.000000    0.000000    0.000000    7.910400   \n",
       "50%      0.000000    3.000000   29.699118    0.000000    0.000000   14.454200   \n",
       "75%      1.000000    3.000000   35.000000    1.000000    0.000000   31.000000   \n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200   \n",
       "\n",
       "             male  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "count  891.000000               891.000000                891.000000   \n",
       "mean     0.647587                 0.188552                  0.086420   \n",
       "std      0.477990                 0.391372                  0.281141   \n",
       "min      0.000000                 0.000000                  0.000000   \n",
       "25%      0.000000                 0.000000                  0.000000   \n",
       "50%      1.000000                 0.000000                  0.000000   \n",
       "75%      1.000000                 0.000000                  0.000000   \n",
       "max      1.000000                 1.000000                  1.000000   \n",
       "\n",
       "       embarked_from_southampton  \n",
       "count                 891.000000  \n",
       "mean                    0.722783  \n",
       "std                     0.447876  \n",
       "min                     0.000000  \n",
       "25%                     0.000000  \n",
       "50%                     1.000000  \n",
       "75%                     1.000000  \n",
       "max                     1.000000  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('titanic_all_numeric.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "891/891 [==============================] - 0s 401us/step - loss: 3.3760 - acc: 0.6117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2dab1d68>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "predictors = df.drop('survived', axis=1).values\n",
    "target = to_categorical(df.survived)\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation='relu', \n",
    "                input_shape = (n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
