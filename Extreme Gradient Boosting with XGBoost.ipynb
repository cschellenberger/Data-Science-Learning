{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Datacamp Course Link](https://www.datacamp.com/courses/extreme-gradient-boosting-with-xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to XGBoost\n",
    "\n",
    "```python\n",
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.20, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n",
    "\n",
    "```\n",
    "\n",
    "### Base Learners\n",
    "- Individual learning algorithm in an ensemble algorithm\n",
    "- CART -- Classification and Regression Tree\n",
    "- each leaf **always** contains a real value score\n",
    "- can later be converted to categories by thresholding if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is boosting?\n",
    "- boosting is a concept that can be applied to many machine learning algorithms\n",
    "- A weak learner is one that is just better than chance\n",
    "- Boosting learns iteratively from a series of weak learners\n",
    "\n",
    "- Model evaluation through cross-validation\n",
    "    - Cross-Validation: Robust method for estimating the performance of a model on unseen data\n",
    "    - Generates many non-overlapping train/test splits on training data\n",
    "    - Reports the average test set performance across all data splits\n",
    "    \n",
    "## Cross validation in XGBoost example\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "class_data = pd.read_csv('classification_data.csv')\n",
    "churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1], label=churn_data.month_5_still_here)\n",
    "\n",
    "parms={\"objective\":\"binary:logistic\", \"max_depth\":4}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=churn_dmartix, params=params, nfold=4, num_boost_round=10, metrics=\"error\", as_pandas=True)\n",
    "\n",
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Two\n",
    "\n",
    "```python\n",
    "# Create the DMatrix: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "```\n",
    "\n",
    "`cv_results` stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From `cv_results`, the final round `'test-error-mean'` is extracted and converted into an accuracy, where accuracy is `1-error`. The final accuracy of around 75% is an improvement from earlier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])\n",
    "```\n",
    "\n",
    "An AUC of 0.84 is quite strong. As you have seen, XGBoost's learning API makes it very easy to compute any metric you may be interested in. In Chapter 3, you'll learn about techniques to fine-tune your XGBoost models to improve their performance even further. For now, it's time to learn a little about exactly when to use XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to NOT use XGBoost\n",
    "\n",
    "- Image regognition\n",
    "- Computer vision\n",
    "- Natural language processing and understanding problems\n",
    "- When the number of training samples is significantly smaller than the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for Regression\n",
    "\n",
    "## Common regression metrics\n",
    "\n",
    "- Root mean squared error (RMSE)\n",
    "    - But it tends to punish large differences between actual and predicted values much more than smaller ones\n",
    "- Mean absolute error (MAE)\n",
    "\n",
    "# Objective (loss) functions and base learners\n",
    "\n",
    "## Objective Functions and Why We Use Them\n",
    "- Quantifies how far off a prediction is from the actual result\n",
    "- Measures the difference between estimated and true values for some collection of data\n",
    "- Goal: Find the model that yields the minimum value of the loss function\n",
    "\n",
    "- Loss functions names in xgboost:\n",
    "    - reg:linear - use for regression problems\n",
    "    - reg:logistic - use for classification problems when you want just decision, not probability\n",
    "    - binary:logistic - use when you want probability rather than just decision\n",
    "    \n",
    "- XGBoost involves creating a meta-model that is composed of many individual models that combine to give a final prediction\n",
    "- Individual models = base learners\n",
    "- Want base learners that when combined create final prediction that is non-linear\n",
    "- Each base learner should be good at distinguishing or predicting different parts of the dataset\n",
    "- Two kinds of base learners: tree and linear\n",
    "\n",
    "# Trees as Base Learners example: Scikit-learn API\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "X, y = boston_data.iloc[:,:-1], boston_data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "```\n",
    "\n",
    "# Linear Base Learners Example: Learning API Only\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "X, y = boston_data.iloc[:,:-1], boston_data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# NEW\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=10)\n",
    "\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_set = \"ames_housing_trimmed_processed.csv\"\n",
    "\n",
    "ames = pd.read_csv(data_set)\n",
    "\n",
    "X, y = ames.iloc[:,:-1], ames.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Installing xgboost on OSX](https://xgboost.readthedocs.io/en/latest/build.html)\n",
    "\n",
    "`brew install gcc@7`\n",
    "Then install XGBoost with pip:\n",
    "\n",
    "`pip3 install xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 78847.401758\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 44331.645061\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0    141767.531250      429.454591   142980.433594    1193.791602\n",
      "1    102832.544922      322.469930   104891.394532    1223.158855\n",
      "2     75872.615235      266.475960    79478.937500    1601.344539\n",
      "3     57245.652343      273.625086    62411.920899    2220.150028\n",
      "4     44401.298828      316.423666    51348.279297    2963.377719\n",
      "4    51348.279297\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[22:44:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
      "0   127343.482421     668.308109  127634.000000   2404.009898\n",
      "1    89770.056641     456.965267   90122.501953   2107.912810\n",
      "2    63580.791016     263.404950   64278.558594   1887.567576\n",
      "3    45633.155274     151.883420   46819.168945   1459.818607\n",
      "4    33587.090820      86.999396   35670.646484   1140.607452\n",
      "4    35670.646484\n",
      "Name: test-mae-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in XGBoost\n",
    "\n",
    "- Regularization is a control on model complexity\n",
    "- Want models that are both accurate and as simple as possible\n",
    "- Regularization parameters in XGBoost:\n",
    "    - gamma - minimum loss reduction allowed for a split to occur\n",
    "    - alpha - l1 regularization on leaf weights, larger values mean more regularization\n",
    "    - lambda - l2 regularization on leaf weights\n",
    "\n",
    "# Base learners in XGBoost\n",
    "\n",
    "- Linear Base Learner:\n",
    "    - Sum of linear terms\n",
    "    - Boosted model is weighted sum of linear models (thus is itself linear)\n",
    "    - Rarely used\n",
    "- Tree Base Learner\n",
    "    - Decision tree\n",
    "    - Boosted model is weighted sum of decision trees (nonlinear)\n",
    "    - Almost exclusively used in XGBoost\n",
    "    \n",
    "# Aside -- Creating DataFrames from multiple equal-length lists\n",
    "\n",
    "- `pd.DataFrame(list(zip(list1, list2)), columns=[\"list1\", \"list2\"])`\n",
    "- `zip` creates a `generator` of parallel values:\n",
    "    - `zip([1,2,3],[\"a\",\"b\",\"c\"])=[1,\"a\"], [2,\"b\"],[3,\"c\"]`\n",
    "    - `generators` need to be completely instantiated before they can be used in `DataFrame` objects\n",
    "- `list()` instantiates the full generator and passing that into the `DataFrame` converts the whole expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
