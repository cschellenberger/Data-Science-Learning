{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Datacamp Course Link](https://www.datacamp.com/courses/extreme-gradient-boosting-with-xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to XGBoost\n",
    "\n",
    "```python\n",
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.20, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n",
    "\n",
    "```\n",
    "\n",
    "### Base Learners\n",
    "- Individual learning algorithm in an ensemble algorithm\n",
    "- CART -- Classification and Regression Tree\n",
    "- each leaf **always** contains a real value score\n",
    "- can later be converted to categories by thresholding if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is boosting?\n",
    "- boosting is a concept that can be applied to many machine learning algorithms\n",
    "- A weak learner is one that is just better than chance\n",
    "- Boosting learns iteratively from a series of weak learners\n",
    "\n",
    "- Model evaluation through cross-validation\n",
    "    - Cross-Validation: Robust method for estimating the performance of a model on unseen data\n",
    "    - Generates many non-overlapping train/test splits on training data\n",
    "    - Reports the average test set performance across all data splits\n",
    "    \n",
    "## Cross validation in XGBoost example\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "class_data = pd.read_csv('classification_data.csv')\n",
    "churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1], label=churn_data.month_5_still_here)\n",
    "\n",
    "parms={\"objective\":\"binary:logistic\", \"max_depth\":4}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=churn_dmartix, params=params, nfold=4, num_boost_round=10, metrics=\"error\", as_pandas=True)\n",
    "\n",
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Two\n",
    "\n",
    "```python\n",
    "# Create the DMatrix: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "```\n",
    "\n",
    "`cv_results` stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From `cv_results`, the final round `'test-error-mean'` is extracted and converted into an accuracy, where accuracy is `1-error`. The final accuracy of around 75% is an improvement from earlier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])\n",
    "```\n",
    "\n",
    "An AUC of 0.84 is quite strong. As you have seen, XGBoost's learning API makes it very easy to compute any metric you may be interested in. In Chapter 3, you'll learn about techniques to fine-tune your XGBoost models to improve their performance even further. For now, it's time to learn a little about exactly when to use XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to NOT use XGBoost\n",
    "\n",
    "- Image regognition\n",
    "- Computer vision\n",
    "- Natural language processing and understanding problems\n",
    "- When the number of training samples is significantly smaller than the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for Regression\n",
    "\n",
    "## Common regression metrics\n",
    "\n",
    "- Root mean squared error (RMSE)\n",
    "    - But it tends to punish large differences between actual and predicted values much more than smaller ones\n",
    "- Mean absolute error (MAE)\n",
    "\n",
    "# Objective (loss) functions and base learners\n",
    "\n",
    "## Objective Functions and Why We Use Them\n",
    "- Quantifies how far off a prediction is from the actual result\n",
    "- Measures the difference between estimated and true values for some collection of data\n",
    "- Goal: Find the model that yields the minimum value of the loss function\n",
    "\n",
    "- Loss functions names in xgboost:\n",
    "    - reg:linear - use for regression problems\n",
    "    - reg:logistic - use for classification problems when you want just decision, not probability\n",
    "    - binary:logistic - use when you want probability rather than just decision\n",
    "    \n",
    "- XGBoost involves creating a meta-model that is composed of many individual models that combine to give a final prediction\n",
    "- Individual models = base learners\n",
    "- Want base learners that when combined create final prediction that is non-linear\n",
    "- Each base learner should be good at distinguishing or predicting different parts of the dataset\n",
    "- Two kinds of base learners: tree and linear\n",
    "\n",
    "# Trees as Base Learners example: Scikit-learn API\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "X, y = boston_data.iloc[:,:-1], boston_data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "```\n",
    "\n",
    "# Linear Base Learners Example: Learning API Only\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "X, y = boston_data.iloc[:,:-1], boston_data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# NEW\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=10)\n",
    "\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_set = \"ames_housing_trimmed_processed.csv\"\n",
    "\n",
    "ames = pd.read_csv(data_set)\n",
    "\n",
    "X, y = ames.iloc[:,:-1], ames.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Installing xgboost on OSX](https://xgboost.readthedocs.io/en/latest/build.html)\n",
    "\n",
    "`brew install gcc@7`\n",
    "Then install XGBoost with pip:\n",
    "\n",
    "`pip3 install xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 78847.401758\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 44331.645061\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0    141767.531250      429.454591   142980.433594    1193.791602\n",
      "1    102832.544922      322.469930   104891.394532    1223.158855\n",
      "2     75872.615235      266.475960    79478.937500    1601.344539\n",
      "3     57245.652343      273.625086    62411.920899    2220.150028\n",
      "4     44401.298828      316.423666    51348.279297    2963.377719\n",
      "4    51348.279297\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4, 'silent':1}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
      "0   127343.482421     668.308109  127634.000000   2404.009898\n",
      "1    89770.056641     456.965267   90122.501953   2107.912810\n",
      "2    63580.791016     263.404950   64278.558594   1887.567576\n",
      "3    45633.155274     151.883420   46819.168945   1459.818607\n",
      "4    33587.090820      86.999396   35670.646484   1140.607452\n",
      "4    35670.646484\n",
      "Name: test-mae-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4, 'silent':1}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in XGBoost\n",
    "\n",
    "- Regularization is a control on model complexity\n",
    "- Want models that are both accurate and as simple as possible\n",
    "- Regularization parameters in XGBoost:\n",
    "    - gamma - minimum loss reduction allowed for a split to occur\n",
    "    - alpha - l1 regularization on leaf weights, larger values mean more regularization\n",
    "    - lambda - l2 regularization on leaf weights\n",
    "\n",
    "# Base learners in XGBoost\n",
    "\n",
    "- Linear Base Learner:\n",
    "    - Sum of linear terms\n",
    "    - Boosted model is weighted sum of linear models (thus is itself linear)\n",
    "    - Rarely used\n",
    "- Tree Base Learner\n",
    "    - Decision tree\n",
    "    - Boosted model is weighted sum of decision trees (nonlinear)\n",
    "    - Almost exclusively used in XGBoost\n",
    "    \n",
    "# Aside -- Creating DataFrames from multiple equal-length lists\n",
    "\n",
    "- `pd.DataFrame(list(zip(list1, list2)), columns=[\"list1\", \"list2\"])`\n",
    "- `zip` creates a `generator` of parallel values:\n",
    "    - `zip([1,2,3],[\"a\",\"b\",\"c\"])=[1,\"a\"], [2,\"b\"],[3,\"c\"]`\n",
    "    - `generators` need to be completely instantiated before they can be used in `DataFrame` objects\n",
    "- `list()` instantiates the full generator and passing that into the `DataFrame` converts the whole expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rmse as a function of l2:\n",
      "    l2          rmse\n",
      "0    1  52275.357421\n",
      "1   10  57746.064453\n",
      "2  100  76624.625000\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3, 'silent':1}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":2, 'silent':1}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xg_reg, num_trees=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the fifth tree\n",
    "xgb.plot_tree(xg_reg, num_trees=4)\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree sideways\n",
    "xgb.plot_tree(xg_reg, num_trees=9, rankdir=\"LR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":2, 'silent':1}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why tune your model?\n",
    "\n",
    "## Untuned Model Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned rmse : 34624.229980\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "untuned_params={\"objective\":\"reg:linear\", 'silent':1}\n",
    "untuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=untuned_params, nfold=4, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "print(\"Untuned rmse : %f\" %((untuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned rmse : 29812.683594\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "tuned_params={\"objective\":\"reg:linear\", 'colsample_bytree': 0.3, 'learning_rate': 0.1, 'max_depth': 5, 'silent':1}\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=tuned_params, nfold=4, num_boost_round=200, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "print(\"Tuned rmse : %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding `'silent':1` to the params dictionary supressed the output to the console on each cross-validation iteration. [xgboost example Github](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/cross_validation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.299479\n",
      "1                   10  34774.194010\n",
      "2                   15  32895.098958\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3, 'silent':1}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0    141871.630208      403.632626   142640.656250     705.559400\n",
      "1    103057.036458       73.769561   104907.664063     111.113862\n",
      "2     75975.966146      253.726099    79262.059895     563.766991\n",
      "3     57420.529948      521.658354    61620.136719    1087.694282\n",
      "4     44552.955729      544.170190    50437.562500    1846.448017\n",
      "5     35763.949219      681.798925    43035.658854    2034.471024\n",
      "6     29861.464844      769.571318    38600.880208    2169.796232\n",
      "7     25994.675781      756.521419    36071.817708    2109.795430\n",
      "8     23306.836588      759.238254    34383.184896    1934.546688\n",
      "9     21459.770833      745.624404    33509.139974    1887.375633\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4, 'silent':1}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, metrics=\"rmse\", early_stopping_rounds=10, as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.406250\n",
      "1  0.010  179932.182292\n",
      "2  0.100   79759.411458\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3, \"silent\":1}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, metrics=\"rmse\", early_stopping_rounds=5, as_pandas=True, seed=123)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37957.468750\n",
      "1          5  35596.599610\n",
      "2         10  36065.546875\n",
      "3         20  36739.576172\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\", \"silent\":1}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, metrics=\"rmse\", early_stopping_rounds=5, num_boost_round=10, as_pandas=True, seed=123)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  44363.458985\n",
      "1               0.5  36266.462890\n",
      "2               0.8  35704.357422\n",
      "3               1.0  35836.046875\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3, \"silent\":1}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awesome! There are several other individual parameters that you can tune, such as `\"subsample\"`, which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Grid Search and Random Search\n",
    "\n",
    "## Grid Search: Review\n",
    "- Search exhaustively over a given set of hyperparameters, once per set of hyperparameters\n",
    "- Number of models = number of distinct values per hyperparameter multiplied across each hyperparameter\n",
    "- Pick final model hyperparameter values that give best cross-validated evaluation metric value\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "gmb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 0.9],\n",
    "    'n_estimators': [200],\n",
    "    'subsample': [0.3, 0.5, 0.9]}\n",
    "\n",
    "gmb = xgb.XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator=gbm,\n",
    "                        param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=4,\n",
    "                        verbose=1)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \",grid_mse.best_params_)\n",
    "\n",
    "print(\"Lowest RMSE found: \"), np.sqrt(np.abs(grid_mse.best_score_))\n",
    "```\n",
    "\n",
    "## Random Search: Review\n",
    "- Create a (possibly infinite) range of hyperparameter values per hyperparameter that you would like to search over\n",
    "- Set the number of iterations you would like for the random search to continue\n",
    "- During each iteration, randomly draw a value in the range of specified values for each hyperparameter searched over and train/evaluate a model with those hyperparameters\n",
    "- After you've reached the maximum number of iterations, select the hyperparameter configuration with the best evaluated score\n",
    "\n",
    "### Random Search Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "gmb_param_grid = {\n",
    "    'learning_rate': np.arange(0.05, 1.05, .05),\n",
    "    'n_estimators': [200],\n",
    "    'subsample': np.arange(0.05, 1.05, .05)}\n",
    "\n",
    "gmb = xgb.XGBRegressor()\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm,\n",
    "                                    param_distributions=gbm_param_grid,\n",
    "                                    n_iter=25,\n",
    "                                    scoring='neg_mean_squared_error',\n",
    "                                    cv=4,\n",
    "                                    verbose=1)\n",
    "# this next step and take some time\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "\n",
    "print(\"Lowest RMSE found: \"), np.sqrt(np.abs(randomized_mse.best_score_))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  29655.33697347771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = grid_mse = GridSearchCV(estimator=gbm,\n",
    "                        param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=4,\n",
    "                        verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 5}\n",
      "Lowest RMSE found:  36636.35808132903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm,\n",
    "                                    param_distributions=gbm_param_grid,\n",
    "                                    n_iter=5,\n",
    "                                    scoring='neg_mean_squared_error',\n",
    "                                    cv=4,\n",
    "                                    verbose=1)\n",
    "\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limits of Grid Search and Random Search\n",
    "\n",
    "- Grid search required time increases exponentially\n",
    "\n",
    "- Random search has an exponentially increasing hyperparameter space, and randomly jumping through an arbitrarily large hyperparameter space may, or may not, present a suitable configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Review\n",
    "\n",
    "- Pipelines are objects that take a list of named 2-tuples as input (name, pipeline_step). A string name as the first input and any scikit-learn compatible transformer or estimator object as the second element.\n",
    "- Pipeline implements fit/predict methods\n",
    "- Can be used as input estimator into grid/randomized search and cross_val_score methods. This is where the true power of piplines is realized. The most useful of which is the `cross_val_score` method that allows for efficient cross-validation and out of sample metric calculation and the grid search and random search approaches for hyperparameter tuning that we just worked through.\n",
    "\n",
    "# Scikit-learn pipeline example\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.ensenble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "names = [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\", \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "\n",
    "data = pd.read_csv(\"boston_housing.csv\", names=names)\n",
    "\n",
    "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "\n",
    "rf_pipeline = Pipeline[(\"st_scaler\", StandardScaler()), (\"rf_model\", RandomForestRegressor())]\n",
    "\n",
    "scores = cross_val_score(rf_pipeline,X,y,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "\n",
    "print(\"Final RMSE: \", final_avg_rmse)\n",
    "```\n",
    "\n",
    "## Preprocessing I: LabelEncoder and OneHotEncoder\n",
    "\n",
    "- LabelEncoder: Converts a categorical column of strings into integers\n",
    "- OneHotEncoder: Takes the column of integers and encodes them as dummy variables\n",
    "\n",
    "**Currently, this process cannot be executed within a pipeline**\n",
    "\n",
    "## Preprocessing II: DictVectorizer\n",
    "\n",
    "- Traditionally used in text processing\n",
    "- Converts lists of feature mappings into vectors\n",
    "- Need to convert DataFrame into a list of dictionary entries\n",
    "- Explore the [scikit-learn documentation](http://scikit-learn.org/stable/documentation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ames_unprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
      "0       RL      CollgCr     1Fam     2Story          Y\n",
      "1       RL      Veenker     1Fam     1Story          Y\n",
      "2       RL      CollgCr     1Fam     2Story          Y\n",
      "3       RL      Crawfor     1Fam     2Story          Y\n",
      "4       RL      NoRidge     1Fam     2Story          Y\n",
      "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
      "0         3             5         0           5           2\n",
      "1         3            24         0           2           2\n",
      "2         3             5         0           5           2\n",
      "3         3             6         0           5           2\n",
      "4         3            15         0           5           2\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.500e+01 8.450e+03\n",
      "  7.000e+00 5.000e+00 2.003e+03 0.000e+00 1.710e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 3.000e+00 0.000e+00 5.480e+02 2.085e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+01 8.000e+01 9.600e+03\n",
      "  6.000e+00 8.000e+00 1.976e+03 0.000e+00 1.262e+03 0.000e+00 1.000e+00\n",
      "  2.000e+00 0.000e+00 3.000e+00 1.000e+00 4.600e+02 1.815e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.800e+01 1.125e+04\n",
      "  7.000e+00 5.000e+00 2.001e+03 1.000e+00 1.786e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 3.000e+00 1.000e+00 6.080e+02 2.235e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 7.000e+01 6.000e+01 9.550e+03\n",
      "  7.000e+00 5.000e+00 1.915e+03 1.000e+00 1.717e+03 1.000e+00 0.000e+00\n",
      "  1.000e+00 0.000e+00 3.000e+00 1.000e+00 6.420e+02 1.400e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 8.400e+01 1.426e+04\n",
      "  8.000e+00 5.000e+00 2.000e+03 0.000e+00 2.198e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 4.000e+00 1.000e+00 8.360e+02 2.500e+05]]\n",
      "(1460, 21)\n",
      "(1460, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded = ohe.fit_transform(df)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded[:5, :])\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing both encoding steps with `DictVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 2.000e+00 5.480e+02\n",
      "  1.710e+03 1.000e+00 5.000e+00 8.450e+03 6.500e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
      " [3.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.600e+02\n",
      "  1.262e+03 0.000e+00 2.000e+00 9.600e+03 8.000e+01 2.000e+01 3.000e+00\n",
      "  2.400e+01 8.000e+00 6.000e+00 2.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 6.080e+02\n",
      "  1.786e+03 1.000e+00 5.000e+00 1.125e+04 6.800e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 1.000e+00 6.420e+02\n",
      "  1.717e+03 0.000e+00 5.000e+00 9.550e+03 6.000e+01 7.000e+01 3.000e+00\n",
      "  6.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
      " [4.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 8.360e+02\n",
      "  2.198e+03 1.000e+00 5.000e+00 1.426e+04 8.400e+01 6.000e+01 3.000e+00\n",
      "  1.500e+01 5.000e+00 8.000e+00 2.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
      "{'MSSubClass': 12, 'MSZoning': 13, 'LotFrontage': 11, 'LotArea': 10, 'Neighborhood': 14, 'BldgType': 1, 'HouseStyle': 9, 'OverallQual': 16, 'OverallCond': 15, 'YearBuilt': 20, 'Remodeled': 18, 'GrLivArea': 7, 'BsmtFullBath': 2, 'BsmtHalfBath': 3, 'FullBath': 5, 'HalfBath': 8, 'BedroomAbvGr': 0, 'Fireplaces': 4, 'GarageArea': 6, 'PavedDrive': 17, 'SalePrice': 19}\n"
     ]
    }
   ],
   "source": [
    "# Import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "df_dict = df.to_dict(\"records\")\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded[:5,:])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>...</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>...</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "      <td>2</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>...</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "      <td>2</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>...</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>2</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>...</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "      <td>2</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>...</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>836</td>\n",
       "      <td>2</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  MSZoning  LotFrontage  LotArea  Neighborhood  BldgType  \\\n",
       "0          60         3         65.0     8450             5         0   \n",
       "1          20         3         80.0     9600            24         0   \n",
       "2          60         3         68.0    11250             5         0   \n",
       "3          70         3         60.0     9550             6         0   \n",
       "4          60         3         84.0    14260            15         0   \n",
       "\n",
       "   HouseStyle  OverallQual  OverallCond  YearBuilt    ...      GrLivArea  \\\n",
       "0           5            7            5       2003    ...           1710   \n",
       "1           2            6            8       1976    ...           1262   \n",
       "2           5            7            5       2001    ...           1786   \n",
       "3           5            7            5       1915    ...           1717   \n",
       "4           5            8            5       2000    ...           2198   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  Fireplaces  \\\n",
       "0             1             0         2         1             3           0   \n",
       "1             0             1         2         0             3           1   \n",
       "2             1             0         2         1             3           1   \n",
       "3             1             0         1         0             3           1   \n",
       "4             1             0         2         1             4           1   \n",
       "\n",
       "   GarageArea  PavedDrive  SalePrice  \n",
       "0         548           2     208500  \n",
       "1         460           2     181500  \n",
       "2         608           2     223500  \n",
       "3         642           2     140000  \n",
       "4         836           2     250000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('ohe_onestep', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('xgb_model', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_ch...\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict(\"records\"), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold RMSE:  30343.486551766466\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age',\n",
    "           'bp',\n",
    "           'sg','al','su','rbc','pc','pcc',\n",
    "           'ba','bgr','bu','sc','sod','pot',\n",
    "           'hemo','pcv','wc','rc','htn','dm',\n",
    "           'cad','appet','pe','ane','class']\n",
    "\n",
    "df = pd.read_csv(\"chronic_kidney_disease.csv\", names=columns)\n",
    "\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-f95a0341a9b6>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-f95a0341a9b6>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    [(numeric_feature, strategy=\"median\") for numeric_feature in non_categorical_columns],\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [(numeric_feature, strategy=\"median\") for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = CategoricalImputer(\n",
    "                                                [(category_feature, strategy=\"ffill\") for category_feature in categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf__max_depth': np.arange(3, 10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline, \n",
    "                                        param_distributions=gbm_param_grid, \n",
    "                                        n_iter=2, \n",
    "                                        scoring=\"roc_auc\", \n",
    "                                        verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we have not covered\n",
    "- Using XGBoost for ranking/recommendation problems (Netfilx/Amazon problem). Achieved through modifying the loss function in specific ways.\n",
    "- Using more sophisticated hyperparameter tuning strategies for tuning XGBoost models (Bayesian Optimization)\n",
    "- Using XGBoost as part of an ensemble of other models for regression/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
