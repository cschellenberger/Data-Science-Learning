{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Fundamentals in Python\n",
    "## [DataCamp Dashboard](https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python)\n",
    "\n",
    "### Regular expressions\n",
    "```python\n",
    "import re\n",
    "\n",
    "re.match('abc', 'abcdef')\n",
    "\n",
    "word_regex = '\\w+'\n",
    "re.match(word_regex, 'hi there!')\n",
    "```\n",
    "- \\w+ -- words\n",
    "- \\d -- digits\n",
    "- \\s -- space\n",
    "- .* -- wildcard\n",
    "- \\+ or * -- greedy match\n",
    "- \\S -- **not** space\n",
    "- \\[a-z\\] lowercase group\n",
    "\n",
    "`re` module\n",
    "`split` -- split a string on regex\n",
    "`findall` -- find all patterns in a string\n",
    "`search` -- search for a pattern\n",
    "`match` -- match an entire string or substring based on a pattern\n",
    "\n",
    "- Pattern first, and the string second\n",
    "- May return an iterator, string, or match object\n",
    "\n",
    "```python\n",
    "re.split('\\s+', 'Split on spaces')\n",
    "```\n",
    "\n",
    "### Regular Expression Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Sting: Let's write RegEx!\n",
      "\n",
      "\n",
      "\\s+ : \n",
      " [' ', ' '] \n",
      "\n",
      "\\w+ : \n",
      " ['Let', 's', 'write', 'RegEx'] \n",
      "\n",
      "[a-z] : \n",
      " ['e', 't', 's', 'w', 'r', 'i', 't', 'e', 'e', 'g', 'x'] \n",
      "\n",
      "\\w : \n",
      " ['L', 'e', 't', 's', 'w', 'r', 'i', 't', 'e', 'R', 'e', 'g', 'E', 'x'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!\"\n",
    "pattern = [r\"\\s+\", r\"\\w+\", r\"[a-z]\", r\"\\w\"]\n",
    "print('My Sting:', \"Let's write RegEx!\\n\\n\")\n",
    "\n",
    "for ent in pattern:\n",
    "    print(str(ent) + ' : \\n', re.findall(ent, my_string), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- Turning a string or document into **tokens** (smaller chunks)\n",
    "- One step in preparing a text for NLP\n",
    "- Many different theories and rules\n",
    "- You can create your own rules using regular expressions\n",
    "- Some examples:\n",
    "    - Breaking out words or sentences\n",
    "    - Separating punctuation\n",
    "    - Separating all hashtags in a tweet\n",
    "    \n",
    "# `nltk` library\n",
    "- `nltk` natural language toolkit\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Hi there!\")\n",
    "```\n",
    "Output:\n",
    "`['Hi', 'there', '!']`\n",
    "\n",
    "- Easier to map part of speech\n",
    "- Matching common words\n",
    "- Removing unwanted tokens\n",
    "\n",
    "# Other `nltk` tokenizers\n",
    "- `sent_tokenize` - tokenize a document into sentences\n",
    "- `regexp_tokenize` - tokenize a string or document based on a regular expression pattern\n",
    "- `TweetTokenizer` - special class just for tweet tokenization, allowing you to separate hashtags, mentions and lots of exclamation points!!!\n",
    "\n",
    "# More regex practice\n",
    "- Difference between `re.search()` and `re.match()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.match('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match('cd', 'abcde') # fails because not at beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(2, 4), match='cd'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grip', 'tropical', 'carrying', 'times', 'wants', 'Not', 'No', 'SCENE', 'Wait', 'master', 'there', 'suggesting', 'empty', 'beat', 'ARTHUR', 'son', 'all', 'order', 'That', 'Saxons', '1', 'sun', 'at', \"'\", 'our', 'matter', 'coconut', 'south', 'not', 'castle', 'your', 'Am', 'husk', 'are', \"'s\", 'King', 'Britons', 'strangers', 'halves', 'Halt', 'trusty', 'maintain', 'wind', ']', 'pound', 'court', 'land', 'Listen', 'Pendragon', \"'em\", 'he', 'you', 'snows', 'speak', 'winter', 'by', 'other', 'knights', 'Are', 'Found', 'me', 'Yes', 'or', 'servant', 'African', 'Oh', 'European', 'got', 'is', 'zone', 'found', 'then', 'will', 'forty-three', 'England', 'since', 'seek', 'every', 'Camelot', \"'ve\", '.', 'mean', 'Who', '2', 'second', 'course', 'air-speed', 'anyway', 'and', 'velocity', 'grips', \"'re\", 'It', 'A', ':', 'Where', 'get', 'sovereign', 'SOLDIER', 'why', 'it', 'this', '#', 'the', 'two', 'Uther', 'carried', 'Ridden', ',', '[', 'weight', 'Pull', 'with', 'dorsal', '...', 'if', 'swallow', 'carry', \"'d\", 'these', 'strand', \"'m\", 'my', 'have', 'search', 'five', 'held', 'Well', 'clop', 'length', 'ridden', 'climes', 'simple', 'bird', 'bangin', 'an', 'horse', 'Whoa', 'join', '--', 'yet', 'am', 'through', 'from', 'kingdom', 'ounce', 'could', 'martin', \"n't\", 'coconuts', 'In', 'go', 'breadth', 'They', 'may', 'fly', 'ask', 'be', '!', 'creeper', 'Patsy', 'Supposing', 'using', 'on', 'line', 'who', 'But', 'I', 'under', 'Please', 'that', 'bring', 'back', 'goes', 'We', 'where', 'its', 'one', 'house', 'warmer', 'together', 'must', 'defeator', 'here', 'The', 'Will', 'Court', 'tell', 'lord', 'needs', 'plover', 'yeah', 'question', 'in', 'feathers', 'So', 'wings', 'guiding', 'ratios', 'they', 'minute', 'interested', 'use', 'but', 'non-migratory', 'of', 'right', 'What', 'a', 'agree', '?', 'KING', 'them', 'You', 'does', 'maybe', 'migrate', 'Mercea', 'swallows', 'temperate', 'just', 'covered', 'to', 'Arthur', 'do', 'point'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/codyschellenberger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<_sre.SRE_Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced tokenization\n",
    "## Regex groups using or `|`\n",
    "- OR is represented using `|`\n",
    "- You can define a group using `()`\n",
    "- You can define explicit character ranges using `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "\n",
    "re.findall(match_digits_and_words, 'He has 11 cats.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pattern | matches | example\n",
    "--------|---------|---------\n",
    "`[A-Za-z]+` | upper and lowercase English alphabet | 'ABCDEFghijk'\n",
    "`[0-9]` | number from 0 to 9 | 9\n",
    "`[A-Za-z\\-\\.]+` | upper and lowercase English alphabet, `-` and `.` | 'My-Website.com'\n",
    "`(a-z)` | a, `-` and z | 'a-z'\n",
    "`(\\s+\\|,)` | spaces or a comma | ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History of Regular Expressions\n",
    "\n",
    "## g/`Regular Expression`/p\n",
    "\n",
    "### Global Regular Expression Print: \"grep\"\n",
    "- grep became a stand-alone program\n",
    "- Became widely used in other Unix programs (awk, vi, emacs, et al.)\n",
    "- Popularity of Unix and grep\n",
    "\n",
    "- grep evolves with the popularity of Unix. Evolution introduces issues with backwards compatibility\n",
    "- egrep: \"extended grep\" `grep -e`\n",
    "- Many programs, many programmers, many changes\n",
    "- **1986**: POSIX (Portable Operating System Interface) (X for Unix)\n",
    "    - Standard to ensure compatibility\n",
    "    - Basic Regular Expressions (BREs) (Essentially `grep`)\n",
    "    - Extended Regular Expressions (EREs) (Essentially `egrep`)\n",
    "- **1986**: Henry Spencer releases a regex library written in C\n",
    "- **1987**: Larry Wall releases Perl\n",
    "    - Quickly becomes the gold standard in how a regular expression engine should work\n",
    "- Perl-compatible languages and programs\n",
    "    - Apache, C/C++, C#/VB/.NET, Java, Javascript, MySQL, PHP, **Python**, Ruby\n",
    "    - PCRE library - Perl Compatibility Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "pattern2 = '(\\\\w+|#\\\\d|\\\\?|!)'\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "regexp_tokenize(my_string, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    "          '#NLP is super fun! <3 #learning',\n",
    "          'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([#@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode ranges for emoji are:\n",
    "\n",
    "`('\\U0001F300'-'\\U0001F5FF')`, \n",
    "\n",
    "`('\\U0001F600-\\U0001F64F')`, \n",
    "\n",
    "`('\\U0001F680-\\U0001F6FF')`, and \n",
    "\n",
    "`('\\u2600'-\\u26FF-\\u2700-\\u27BF')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
      "['Wann', 'Pizza', 'Und', 'Über']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-Z|Ü]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charting word length with NLTK\n",
    "\n",
    "## Combining NLP data extraction with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "\n",
    "word_lengths = [len(w) for w in words]\n",
    "\n",
    "plt.hist(word_lengths)\n",
    "plt.xlabel('Word Length')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Word Length Distribution for\\n\"This is a pretty cool tool!\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run holy-grail.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65003"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(holy_grail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "plt.xlabel('Line Length')\n",
    "plt.ylabel('Number of Lines')\n",
    "plt.title('Line Length Distribution for\\nScene 1 of Holy Grail')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word counts with bag-of-words\n",
    "- Basic method for finding topics in a text\n",
    "- Need to first create tokens using tokenization\n",
    "- ...and then count up all the tokens\n",
    "- The more frequent a word, the more important it might be\n",
    "- Can be a great way to determine the significant words in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 3, 'cat': 3, 'the': 3, 'box': 3, '.': 3, 'is': 2, 'in': 1, 'likes': 1, 'over': 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run article.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why preprocess?\n",
    "\n",
    "- Helps make for better input data\n",
    "    - When performing machine learning or other statistical methods\n",
    "- Examples:\n",
    "    - Tokenization to create a bag of words\n",
    "    - Lowercasing words\n",
    "- Lemmatization/Stemming\n",
    "    - Shorten words to their root stems\n",
    "- Removing stop words, punctuation, or unwanted tokens\n",
    "- Good to experiment with different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/codyschellenberger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " 'should',\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " 'couldn',\n",
    " 'didn',\n",
    " 'doesn',\n",
    " 'hadn',\n",
    " 'hasn',\n",
    " 'haven',\n",
    " 'isn',\n",
    " 'ma',\n",
    " 'mightn',\n",
    " 'mustn',\n",
    " 'needn',\n",
    " 'shan',\n",
    " 'shouldn',\n",
    " 'wasn',\n",
    " 'weren',\n",
    " 'won',\n",
    " 'wouldn',\n",
    " '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/codyschellenberger/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to gensim\n",
    "- Popular open source NLP library\n",
    "- Uses top academic models to perform complex tasks\n",
    "     - Build document or word vectors\n",
    "     - Performing topic identification and document comparison\n",
    "- [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a statistical model used for topic analysis.\n",
    "\n",
    "# Creating a gensim dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'movie': 5,\n",
       " 'spaceship': 6,\n",
       " 'the': 7,\n",
       " 'was': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'liked': 11,\n",
       " 'really': 12,\n",
       " ',': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'is': 25,\n",
       " 'space': 26,\n",
       " 'more': 27,\n",
       " 'please': 28}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "               'I really liked the movie!',\n",
    "               'Awesome action scenes, but boring characters.',\n",
    "               'The movie was awful! I hate alien films.',\n",
    "               'Space is cool! I liked the movie.',\n",
    "               'More space films, please!']\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between `gensim` and simple bow.\n",
    "\n",
    "- `gensim` models can be easily saved, updated, and reused\n",
    "\n",
    "# What are word vectors?\n",
    "\n",
    "- Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "path = \"/Users/codyschellenberger/Data_science/\"\n",
    "output = []\n",
    "for filename in os.listdir(path):\n",
    "    if re.match(\"wiki_text_\\w+.txt\", filename):\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            text = []\n",
    "            for line in f:\n",
    "                text.append(line)\n",
    "            output.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'''Exception handling'''  is the process of responding to the occurrence, during computation, of ''exceptions'' – anomalous or exceptional conditions requiring special processing – often changing the normal flow of computer program|program execution (computing)|execution. It is provided by specialized programming language constructs or computer hardware mechanisms.\\n\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-ae6ec1d11886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Convert the tokens into lowercase: lower_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlower_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Retain alphabetic words: alpha_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-ae6ec1d11886>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Convert the tokens into lowercase: lower_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlower_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Retain alphabetic words: alpha_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "\n",
    "for article_list in output:\n",
    "    \n",
    "    print(type(article_list))\n",
    "\n",
    "    # Tokenize the article: tokens\n",
    "    wiki_tokens = [word_tokenize(t) for t in article_list]\n",
    "\n",
    "    print(type(wiki_tokens[0]))\n",
    "    \n",
    "    # Convert the tokens into lowercase: lower_tokens\n",
    "    lower_tokens = [t.lower() for t in wiki_tokens]\n",
    "\n",
    "    # Retain alphabetic words: alpha_only\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "    # Remove all stop words: no_stops\n",
    "    art = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "    text_addition.append(art)\n",
    "        \n",
    "    articles.append(text_addition)\n",
    "    \n",
    "type(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary([articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n"
     ]
    }
   ],
   "source": [
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create a MmCorpus: corpus\n",
    "corpus = dictionary.doc2bow(articles)\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[3][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf with gensim\n",
    "- Term frequency - inverse document frequency\n",
    "- Allows you to determine the most important words in each document\n",
    "- Each corpus may have shared words beyond just stopwords\n",
    "- These words should be down-weighted in importance\n",
    "- Example from astronomy: \"Sky\"\n",
    "- Ensures most common words don't show up as key words\n",
    "- Keeps document specific frequency words weighted high\n",
    "$$w_{i,j} = tf_{i,j} * log \\left ( \\frac{N}{df_{i}} \\right )$$\n",
    "$$w_{i,j} = \\text{tf-idf weight for token } i \\text{ in document } j$$\n",
    "$$tf_{i,j} = \\text{number of occurences of token } i \\text{ in document } j$$\n",
    "$$df_{i} = \\text{number of documents that contain token } i$$\n",
    "$$N = \\text{total number of documents}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tdfidfmodel import TfidModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is tf-idf?\n",
    "\n",
    "You want to calculate the tf-idf weight for the word \"computer\", which appears five times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word \"computer\", tf-idf can be calculated by multiplying term frequency with inverse document frequency.\n",
    "\n",
    "Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term.\n",
    "\n",
    "$$\\left ( \\frac{5}{100} \\right ) * log \\left ( \\frac{200}{20} \\right )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "- NLP task to identify important named entities in the text\n",
    "    - People, places, organizations\n",
    "    - Dates, states, works of art\n",
    "    - ... and other categories!\n",
    "- Can be used alongside topic identification\n",
    "    - ... or on its own!\n",
    "- Who? What? When? Where?\n",
    "\n",
    "# nltk and the Stanford CoreNLP Library\n",
    "- The Stanford CoreNLP library:\n",
    "    - Integrate into Python via nltk\n",
    "    - Java based\n",
    "    - Support for NER as well as coreference and dependency trees\n",
    "    \n",
    "# Using nltk for Named Entity Recognition\n",
    "- `NNP` Proper Noun Singular\n",
    "- `GPE` Geo Political Entity\n",
    "- `ORGANIZATION`\n",
    "- `PERSON`\n",
    "    - Using trained grammatical and statistical parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/codyschellenberger/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = '''In New York, I like to ride the Metro to visit MOMA \n",
    "              and some restaurants rated well by Ruth Reichl.'''\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "tagged_sent[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/codyschellenberger/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/codyschellenberger/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "(S\n",
      "  In/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  ride/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (ORGANIZATION MOMA/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restaurants/NNS\n",
      "  rated/VBN\n",
      "  well/RB\n",
      "  by/IN\n",
      "  (PERSON Ruth/NNP Reichl/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the Stanford library with NLTK, NLTK, the Stanford Java Libraries and some environment variables to help with integration are needed to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to SpaCy\n",
    "- NLP library similar to gensim, with different implementations\n",
    "- Focus on creating NLP pipelines to generate models and corpora\n",
    "- Open-source, with extra libraries and tools\n",
    "    - Displacy [Interactive Demo](https://demos.explosion.ai/displacy-ent/)\n",
    "# SpaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.EntityRecognizer at 0x1a1cb08518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "nlp.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Berlin, Germany, \n",
       "              , Angela Merkel)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"\"\"Berlin is the capital of Germany;\n",
    "             and the residence of Chancellor Angela Merkel.\"\"\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin GPE\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents[0], doc.ents[0].label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use SpaCy for NER?\n",
    "- Easy pipline creation\n",
    "- Different entity types compared to `nltk`\n",
    "- Informal laguage corpora\n",
    "    - Easily find entities in Tweets and chat messages\n",
    "- Quickly growing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual NER with polyglot\n",
    "- NLP library which uses word vectors\n",
    "- Why `polyglot`?\n",
    "    - Vectors for +130 different languages\n",
    "# Spanish NER with polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "text = \"\"\"El presidente de la Generalitat de Cataluña,\n",
    "                  Carles Puigdemont, ha afirmado hoy a la alcaldesa \n",
    "                  de Madrid, Manuela Carmena, que en su etapa de \n",
    "                  alcalde de Girona (de julio de 2011 a enero de 2016) \n",
    "                  hizo una gran promoción de Madrid.\"\"\"\n",
    "ptext = Text(text)\n",
    "ptext.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`I-ORG` represents an organization\n",
    "`I-LOC` represents a location\n",
    "`I-PER` represents a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    if \"Márquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying fake news using supervised learning with NLP\n",
    "- What is supervised learning?\n",
    "- Form of machine learning\n",
    "    - Problem has predefined training data\n",
    "    - This data has a label (or outcome) you want the model to learn\n",
    "    - Classification problems\n",
    "    - Goal: Make good hypotheses about the species based on geometric features\n",
    "    \n",
    "# Supervised learning with NLP\n",
    "- Need to use language instead of geometric features\n",
    "- `scikit-learn`: Powerful open-source library\n",
    "- How to create supervised learning data from text?\n",
    "    - Use bag-of-words models or tf-idf as features\n",
    "    \n",
    "# Supervised learning steps\n",
    "- Collect and preprocess our data\n",
    "- Determine a label (Example: Movie genre)\n",
    "- Split data into training and test sets\n",
    "- Extract features from the text to help predict the label\n",
    "    - Bag-of-words vector built into scikit-learn\n",
    "- Evaluate trained model using the test set\n",
    "\n",
    "# Building word count vectors with scikit-learn\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df = ... # Load data into DataFrame\n",
    "y = df['Sci-Fi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['plot'], y, test_size=0.33, random_state=53)\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing a classification model with scikit-learn\n",
    "## Naive Bayes classifier\n",
    "- Naive Bayes Model\n",
    "    - Commonly used for testing NLP classification problems\n",
    "    - Basis in probability\n",
    "    Given a particular piece of data, how likely is a particular outcome?\n",
    "- Examples:\n",
    "    - If the plot has a spaceship, how likely is it to be sci-fi?\n",
    "    - Given a spaceship and an alien, how likely now is it sci-fi?\n",
    "- Each word from CountVectorizer acts as a feature\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "metrics.accuracy_score(y_test, pred)\n",
    "```\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "```python\n",
    "metrics.confusion_matrix(y_test, pred, labels=[0,1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NLP, complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, 0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
